# UNETR Transfer Learning Configuration for BraTS Synthesis

# Inherit from base config
extends: "unetr_base.yaml"

transfer_learning:
  enabled: true
  pretrained_path: "pretrained/brats_segmentation.pth"
  freeze_strategy: "gradual"  # "none", "partial", "gradual"
  freeze_layers: 8
  unfreeze_schedule:
    epoch_10: 6
    epoch_20: 4  
    epoch_30: 0
  
  # Different learning rates for different components
  lr_multipliers:
    frozen_layers: 0.0
    encoder: 0.01
    decoder: 0.1
    synthesis_head: 1.0

training:
  # Override base training config for transfer learning
  learning_rate: 5e-5  # Lower learning rate for transfer learning
  epochs: 150
  scheduler: "step"
  scheduler_params:
    step_size: 30
    gamma: 0.5
  warmup_epochs: 5

loss:
  # Optimized loss weights for transfer learning
  name: "combined"
  mse_weight: 0.8
  ssim_weight: 0.3
  perceptual_weight: 0.2
  l1_weight: 0.1

validation:
  frequency: 5  # More frequent validation for transfer learning
  metrics: ["mse", "ssim", "psnr", "lpips"]
  save_best: true
  early_stopping_patience: 30

logging:
  use_wandb: true
  project_name: "unetr-brats-synthesis-transfer"
  tags: ["transfer_learning", "brats_pretrained"]
