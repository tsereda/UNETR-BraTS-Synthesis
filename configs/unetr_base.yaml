# UNETR Base Configuration for BraTS Synthesis

model:
  name: "UNETR_Synthesis"
  img_size: [96, 96, 96]
  in_channels: 4
  out_channels: 1
  hidden_size: 768
  mlp_dim: 3072
  num_heads: 12
  #num_layers: 12
  feature_size: 16
  norm_name: "instance"
  dropout_rate: 0.1

data:
  modalities: ["t1n", "t1c", "t2w", "t2f"]
  volume_size: [96, 96, 96]
  num_workers: 4
  cache_rate: 0.1
  target_modality: null  # null for random selection during training
  
training:
  batch_size: 2
  learning_rate: 1e-4
  epochs: 200
  optimizer: "AdamW"
  weight_decay: 1e-4
  scheduler: "cosine"
  mixed_precision: true
  gradient_checkpointing: false
  accumulate_grad_batches: 1
  
loss:
  name: "combined"
  mse_weight: 1.0
  ssim_weight: 0.2
  perceptual_weight: 0.1
  l1_weight: 0.1
  
transfer_learning:
  enabled: false
  pretrained_path: null
  freeze_layers: 0
  lr_multipliers:
    encoder: 1.0
    decoder: 1.0
    synthesis_head: 1.0

validation:
  frequency: 10
  metrics: ["mse", "ssim", "psnr", "lpips"]
  save_best: true
  early_stopping_patience: 50

logging:
  use_wandb: false
  project_name: "unetr-brats-synthesis"
  log_frequency: 100
  save_frequency: 1000

hardware:
  gpus: 1
  precision: 16  # 16 for mixed precision, 32 for full precision
  num_workers: 4
